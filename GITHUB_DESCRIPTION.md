# GitHub Project Description

## Short Description (125 chars for GitHub About section)
```
Multi-cloud document archiving with vector search, deep archive support, and Kafka event publishing.
```

## Full Description (for GitHub About/Readme)

**Cloud Document Archive** is a powerful, scalable document management system that enables organizations to archive, retrieve, and intelligently search documents across multiple cloud storage providers with semantic search capabilities.

### Overview

Archive documents with metadata and tags to AWS S3, Azure Blob Storage, or Google Cloud Storage. Retrieve them by unique identifiers, perform semantic searches using AI embeddings, generate analytics reports, and automatically move old documents to cost-effective deep archive storage tiers (Glacier, Archive, etc.) with Kafka event integration.

### Key Features

üóÇÔ∏è **Multi-Cloud Support**
- AWS S3 with Glacier Deep Archive
- Azure Blob Storage with Archive tier
- Google Cloud Storage with Archive class

üìÑ **Document Management**
- Upload documents in base64 format
- Tag documents with custom key-value pairs
- Store rich metadata alongside documents
- Generate unique hashed identifiers

üîç **Intelligent Search**
- Semantic search using AI embeddings (sentence-transformers)
- Find documents by meaning, not just keywords
- Similarity-based ranking with configurable thresholds
- Natural language queries

‚ùÑÔ∏è **Lifecycle Management**
- Automatic archival based on document age
- Move to archive after N days
- Move to deep archive after M days
- Cost optimization through intelligent tiering

üìä **Analytics & Reporting**
- Generate metrics for time periods
- Document distribution by content type
- Tag-based analytics
- Storage usage tracking

üí¨ **Event Integration**
- Kafka publisher for document events
- Archive, delete, restore event streams
- Real-time event notifications
- Integration with workflow systems

üê≥ **Container Ready**
- Docker and Docker Compose configuration
- Kafka integration pre-configured
- SQLite for metadata (easily swappable)
- Production-ready setup

### Technology Stack

**Backend**
- FastAPI 0.109+ - High-performance Python web framework
- Uvicorn - ASGI web server
- SQLAlchemy 2.0+ - ORM with async support

**Cloud & Storage**
- boto3/botocore - AWS S3 and Glacier
- azure-storage-blob - Azure Blob Storage
- google-cloud-storage - GCP Storage

**AI & Search**
- sentence-transformers - Semantic embeddings
- scikit-learn - Similarity calculations
- numpy - Numerical computing

**Message Queue**
- Apache Kafka - Event streaming
- aiokafka/kafka-python - Python clients

**Database & ORM**
- SQLAlchemy 2.0 - Database abstraction
- SQLite (default) - Lightweight metadata storage

**Deployment**
- Docker - Containerization
- Docker Compose - Orchestration

### Quick Start

#### Local Installation
```bash
# Clone repository
git clone <repo-url>
cd clouddocumentachieve

# Create virtual environment
python -m venv venv
source venv/bin/activate  # or `venv\Scripts\activate` on Windows

# Install dependencies
pip install -r requirements.txt

# Configure with environment variables or YAML
export STORAGE_PROVIDER=aws_s3
export AWS_ACCESS_KEY_ID=your-key
export AWS_SECRET_ACCESS_KEY=your-secret

# Run the application
uvicorn app.main:app --reload --host 0.0.0.0 --port 8000
```

#### Docker Deployment
```bash
# Start all services (API, Kafka, Zookeeper)
docker-compose up -d

# API available at http://localhost:8000
```

### Core API Endpoints

**Document Operations**
- `POST /api/v1/archive` - Archive a document
- `GET/POST /api/v1/retrieve/{id}` - Retrieve a document
- `DELETE /api/v1/archive/{id}` - Delete a document
- `POST /api/v1/search` - Semantic search

**Archive Management**
- `POST /api/v1/deep-archive` - Move to deep archive
- `POST /api/v1/restore` - Restore from archive
- `GET /api/v1/archive-status/{id}` - Check archive status
- `POST /api/v1/glacier/retrieve` - Retrieve from Glacier

**Analytics**
- `POST /api/v1/report` - Generate time-period report
- `GET /health` - Health check with statistics

**Lifecycle**
- `POST /api/v1/lifecycle/archive` - Trigger archival
- `POST /api/v1/lifecycle/check-restores` - Check restore status
- `GET /api/v1/lifecycle/eligible` - List eligible documents

### Configuration

**Environment Variables**
```bash
# Storage
STORAGE_PROVIDER=aws_s3              # aws_s3, azure_blob, gcp_storage
AWS_REGION=us-east-1
AWS_S3_BUCKET=document-archive
AZURE_CONNECTION_STRING=...
GCP_PROJECT_ID=...

# Kafka
KAFKA_ENABLED=true
KAFKA_BOOTSTRAP_SERVERS=localhost:9092

# Lifecycle
LIFECYCLE_ENABLED=true
LIFECYCLE_ARCHIVE_AFTER_DAYS=90
LIFECYCLE_DEEP_ARCHIVE_AFTER_DAYS=365
```

**YAML Configuration**
```yaml
storage:
  provider: aws_s3
  aws:
    region: us-east-1
    bucket: document-archive

kafka:
  enabled: true
  bootstrap_servers: localhost:9092

lifecycle:
  enabled: true
  archive_after_days: 90
```

### Vector Search

Find semantically similar documents using AI-powered embeddings:

```bash
curl -X POST http://localhost:8000/api/v1/search \
  -H "Content-Type: application/json" \
  -d '{
    "query": "quarterly financial report",
    "top_k": 5,
    "min_similarity": 0.5
  }'
```

**How It Works**
1. Documents are embedded on archive using `all-MiniLM-L6-v2` model
2. Query is embedded using the same model
3. Cosine similarity calculates relevance
4. Results ranked and filtered by threshold

**Use Cases**
- Find related documents without exact keywords
- Cross-language document discovery
- Duplicate document detection
- Content-based retrieval

### Kafka Events

Stream document lifecycle events to Kafka:

```json
{
  "event_type": "document_archived",
  "document_id": "a1b2c3d4...",
  "filename": "report.pdf",
  "timestamp": "2026-02-17T10:30:00"
}
```

**Event Types**
- `document_archived` - Document uploaded and stored
- `document_deleted` - Document permanently deleted
- `document_moved_to_glacier` - Moved to deep archive
- `document_restore_initiated` - Restore request started
- `document_restore_ready` - Document restored and available

### Testing

**Postman Collection**
Import `Cloud_Document_Archive.postman_collection.json` in Postman for complete API testing with pre-configured examples.

**Example Workflow**
1. Archive a document ‚Üí receive `document_id`
2. Search for similar documents
3. Retrieve the document
4. Move to deep archive
5. Track with health endpoint

### Performance

- **Embedding**: ~2-3 seconds for first model load, <100ms per document after
- **Search**: Typically <500ms for 1000 documents
- **Archive**: <1 second for cloud upload
- **API Response**: <100ms for retrieve operations

### Deployment Architectures

**Standalone**: Single container with SQLite
**Distributed**: Docker Compose with Kafka cluster
**Cloud Native**: Kubernetes with managed cloud storage

### Documentation

- [README.md](README.md) - Full documentation and API reference
- [POSTMAN.md](POSTMAN.md) - Postman collection guide
- [config.yaml.example](config.yaml.example) - Configuration template
- [Swagger UI](http://localhost:8000/docs) - Interactive API documentation

### Contributing

Contributions welcome! Please follow these guidelines:

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/AmazingFeature`)
3. Commit changes (`git commit -m 'Add AmazingFeature'`)
4. Push to branch (`git push origin feature/AmazingFeature`)
5. Open a Pull Request

### License

MIT License - see LICENSE file for details

### Project Status

‚úÖ Production Ready
- ‚úÖ Multi-cloud support
- ‚úÖ Lifecycle management
- ‚úÖ Vector search
- ‚úÖ Kafka integration
- ‚úÖ Docker deployment
- ‚úÖ Comprehensive API
- ‚úÖ Health monitoring

### Support & Issues

For issues, questions, or feature requests, please open a GitHub issue with:
- Description of the issue/request
- Steps to reproduce (if applicable)
- Environment (OS, Python version, cloud provider)
- Relevant logs or error messages

### Roadmap

üöÄ **Planned Features**
- [ ] Multi-tenancy support
- [ ] Policy-based retention
- [ ] Advanced encryption options
- [ ] DLP (Data Loss Prevention) integration
- [ ] Full-text search with Elasticsearch
- [ ] Web-based management console
- [ ] GraphQL API
- [ ] Webhook support

### Credits

Built with ‚ù§Ô∏è using FastAPI, SQLAlchemy, and cloud storage SDKs.

---

**Made for organizations that take document management seriously.**
